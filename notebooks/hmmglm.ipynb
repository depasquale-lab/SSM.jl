{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `\\Users\\ryansenne\\Documents\\Github\\SSM`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\"/Users/ryansenne/Documents/Github/SSM/\")\n",
    "# Pkg.activate(\"/home/ryansenne/PycharmProjects/ssm_julia/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"/Users/ryansenne/Documents/Github/SSM//src/SSM.jl\")\n",
    "# include(\"/home/ryansenne/PycharmProjects/ssm_julia/src/SSM.jl\")\n",
    "using Random\n",
    "using .SSM\n",
    "using Distributions\n",
    "using Plots\n",
    "using LinearAlgebra\n",
    "using ForwardDiff\n",
    "using StatsFuns\n",
    "using Optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{Float64}:\n",
       " -0.2254724389883398\n",
       "  1.1635789534220071"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# generate synthetic data\n",
    "Random.seed!(123)\n",
    "\n",
    "# set parameters\n",
    "n = 100\n",
    "β = [0.1, 0.9]\n",
    "\n",
    "# generate data\n",
    "X = hcat(ones(n), randn(n, 1))\n",
    "\n",
    "p = logistic.(X * β)\n",
    "y = rand.(Bernoulli.(p));\n",
    "\n",
    "logreg = SSM.BernoulliRegression()\n",
    "SSM.fit!(logreg, reshape(X[:, 2], length(y), 1), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{Float64}:\n",
       " -5.0\n",
       " 19.769322685726184"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "objective_gradient!(zeros(2), [0., 0.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{Float64}:\n",
       " -0.22547243895932628\n",
       "  1.1635789534102206"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = optimize(objective, objective_gradient!, [0., 0.], LBFGS())\n",
    "res.minimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = SSM.BernoulliRegression()\n",
    "SSM.fit!(logreg, reshape(X[:, 2], length(y), 1), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal β: [-57.356469034600124, 31.364423201425744]\n"
     ]
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "using StatsFuns: logistic\n",
    "\n",
    "function loglikelihood(β::AbstractVector, X::Matrix{Float64}, y::Union{Vector{Float64}, BitVector}, w::Float64=1.0)\n",
    "    # Calculate log likelihood\n",
    "    p = logistic.(X * β)\n",
    "    # Convert y if necessary\n",
    "    y = convert(Vector{Float64}, y)\n",
    "    return sum(w .* (y .* log.(p) .+ (1 .- y) .* log.(1 .- p)))\n",
    "end\n",
    "\n",
    "function gradient(β::AbstractVector, X::Matrix{Float64}, y::Union{Vector{Float64}, BitVector}, w::Float64=1.0)\n",
    "    # Calculate probabilities\n",
    "    p = logistic.(X * β)\n",
    "    # Convert y if necessary\n",
    "    y = convert(Vector{Float64}, y)\n",
    "    # Compute the gradient\n",
    "    return X' * (y .- p) .* w\n",
    "end\n",
    "\n",
    "# Example usage with Optim.jl\n",
    "using Optim\n",
    "\n",
    "function optimize_loglikelihood(X::Matrix{Float64}, y::Union{Vector{Float64}, BitVector}, w::Float64=1.0)\n",
    "    # Objective function for Optim\n",
    "    function obj(β)\n",
    "        -loglikelihood(β, X, y, w) # we minimize, so take the negative log likelihood\n",
    "    end\n",
    "\n",
    "    # Gradient function for Optim\n",
    "    function grad!(g, β)\n",
    "        g .= -gradient(β, X, y, w) # we minimize, so take the negative gradient\n",
    "    end\n",
    "\n",
    "    # Initial guess for β\n",
    "    β0 = zeros(size(X, 2))\n",
    "\n",
    "    # Perform optimization using L-BFGS\n",
    "    result = optimize(obj, grad!, β0, LBFGS())\n",
    "    return result\n",
    "end\n",
    "\n",
    "# Example data\n",
    "X = [1.0 1.0; 1.0 2.0; 1.0 3.0]\n",
    "y = [0.0, 1.0, 1.0]\n",
    "\n",
    "# Perform optimization\n",
    "result = optimize_loglikelihood(X, y)\n",
    "println(\"Optimal β: \", result.minimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
